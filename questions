a) Consider the following transition probabilities and the reward for an MDP with states 1 and $2. Actions af and a2 are available in each of the states

Compute x as below. x (last digit of your student id +43% 20. No marks will be given for the use of any other Consider the initial values of st, s2, and s3 are 10, 1.0 and 0.0, respectively. Compute values of states after 2 iterations, given the discount factor as x using an asynchronous version of value iteration. Show all the computations 14 M What I will be the optimal policy after the two iterations 105 MI
Compute x as below: x=(last digit of your student Id +4)% 20. No marks 4-86509-2023/1079 202 will be gheen for the use of any other x Consider the initial values of s1, s2, and s3 are 10, 1.0 and 0.0, respectively Compute values of states after 2 iterations, given the discount factor as x using an asynchronous version of value iteration. Show all the computations [4 M). What will be the optimal policy after the two iterations [0.5 M] ? Are the computer values 16509-2023/10/29

05384-86509-2023/10/29-2022aa05

03

04 10

10.0

01210-20221805384-86509-2023/10/20

0

(b converged? Why (or why not)? [0.5 M] ) Only handwritten answer accepted] Referring to the discussions on tile coding. The tilings we considered in the class gave equal importance to all the dimensions. How do you adapt this to situations where certain attributes are more critical to the task than others? Explain a way of doing this in your own language
